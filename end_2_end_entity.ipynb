{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "end_2_end_entity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsWxQTwHt-sB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNsX1GHmuCfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp kaggle.json /root/.kaggle/\n",
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d PromptCloudHQ/flipkart-products\n",
        "! unzip flipkart-products.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnPU5GIVVXqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this covers company names, address and location\n",
        "! wget http://download.companieshouse.gov.uk/BasicCompanyDataAsOneFile-2020-06-01.zip\n",
        "! unzip BasicCompanyDataAsOneFile-2020-06-01.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4D3rcrDVb0L",
        "colab_type": "text"
      },
      "source": [
        "### Gather datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSD2-oXVbYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# company, address and location dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv('BasicCompanyDataAsOneFile-2020-06-01.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XiFmTcUYD7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "df_add = df[[\"RegAddress.AddressLine1\", \" RegAddress.AddressLine2\", \"RegAddress.PostTown\", \"RegAddress.County\", \"RegAddress.Country\", \"RegAddress.PostCode\"]]\n",
        "df_add[\"add\"] = df_add.apply(lambda x : x.to_string(index=False, na_rep=\"\"),axis=1).replace({\"\\n\":','}, regex=True)\n",
        "df_loc = df[[\"RegAddress.PostTown\", \"RegAddress.County\", \"RegAddress.Country\"]]\n",
        "df_loc[\"loc\"] = df_loc.apply(lambda x : x.to_string(index=False, na_rep=\"\"),axis=1).replace({\"\\n\":','}, regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3YolkQkXBUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "names = df.CompanyName.unique()\n",
        "company_df = pd.DataFrame(columns=[\"text\", \"labels\"])\n",
        "company_df[\"text\"] = names\n",
        "company_df[\"labels\"] = [\"CompanyName\"] * len(names)\n",
        "add = df_add[\"add\"].unique()\n",
        "add_df = pd.DataFrame(columns=[\"text\", \"labels\"])\n",
        "add_df[\"text\"] = add\n",
        "add_df[\"labels\"] = [\"CompanyAdd\"] * len(add)\n",
        "loc = df_loc[\"loc\"].unique()\n",
        "loc_df = pd.DataFrame(columns=[\"text\", \"labels\"])\n",
        "loc_df[\"text\"] = loc\n",
        "loc_df[\"labels\"] = [\"CompanyLoc\"] * len(loc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFeHMiqjtNYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# goods\n",
        "df = pd.read_csv(\"flipkart_com-ecommerce_sample.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHokq7W2vBMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "goods = []\n",
        "cat = df[\"product_category_tree\"].apply(lambda x : x.replace(\"[\", \"\").replace(\"]\", \"\").replace('\\\"', \"\").split(\" >> \"))\n",
        "flat_cat = [item for subitem in cat for item in subitem]\n",
        "for x in flat_cat:\n",
        "    if x not in goods:\n",
        "        goods.append(x)\n",
        "goods_df = pd.DataFrame(columns=[\"text\", \"labels\"])\n",
        "goods_df[\"text\"] = goods\n",
        "goods_df[\"labels\"] = [\"Goods\"] * len(goods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3kn1VzXzEPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# serial number\n",
        "import random\n",
        "random.seed(42)\n",
        "def artificial_serials():\n",
        "    start = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "    middle = '|-/'\n",
        "    st = []\n",
        "    for _ in range(15000):\n",
        "        s = \"\"\n",
        "        for i in range(random.randint(3, 5)):\n",
        "            s += random.choice(list(start))\n",
        "        mid = random.choice(list(middle))\n",
        "        s += mid\n",
        "        for j in range(random.randint(2, 5)):\n",
        "            s += random.choice(list(start))\n",
        "        s += mid\n",
        "        for j in range(random.randint(2, 4)):\n",
        "            s += random.choice(list(start))\n",
        "        st.append(s)\n",
        "    start = \"0123456789\"\n",
        "    for _ in range(1000):\n",
        "        s = \"\"\n",
        "        for i in range(random.randint(6, 10)):\n",
        "            s += random.choice(list(start))\n",
        "        st.append(s)\n",
        "    start = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "    for _ in range(1000):\n",
        "        s = \"\"\n",
        "        for i in range(random.randint(3, 5)):\n",
        "            s += random.choice(list(start))\n",
        "        st.append(s)\n",
        "        mid = random.choice(list(middle))\n",
        "        s += mid\n",
        "        for j in range(random.randint(3, 5)):\n",
        "            s += random.choice(list(start))\n",
        "    return st"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQMpqpge08FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "st = artificial_serials()\n",
        "serial = []\n",
        "for s in st:\n",
        "    serial.append(s)\n",
        "serial_df = pd.DataFrame(columns=[\"text\", \"labels\"])\n",
        "serial_df[\"text\"] = serial\n",
        "serial_df[\"labels\"] = [\"Serial\"] * len(serial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHPhNx4KtNcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "97652eb6-8a02-4a16-aea3-ae4f72b9edf6"
      },
      "source": [
        "data = pd.concat([company_df, add_df, loc_df, goods_df, serial_df], ignore_index=True)\n",
        "data.head()\n",
        "print (data[\"labels\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CompanyName    4594492\n",
            "CompanyAdd     2601715\n",
            "CompanyLoc      114019\n",
            "Serial           17000\n",
            "Goods             9045\n",
            "Name: labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs1-9tcgtQRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv(\"dataset.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if17-tt5tXd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# ! cp \"/content/dataset.csv\" \"/content/drive/My Drive/Colab Notebooks/dataset.csv\"\n",
        "! cp \"/content/drive/My Drive/Colab Notebooks/dataset.csv\" \"/content/dataset.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoVEparJbn0e",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUvSGKztM2gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO9ul0NDbm5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from unidecode import unidecode\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopset = list(string.punctuation)\n",
        "stopset.remove(\"&\")\n",
        "stopset.remove(\"-\")\n",
        "stopset.remove(\"/\")\n",
        "stopset.remove(\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGuZZexoMkz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4eb7e8b9-8ddf-4f6e-8ef1-901329ee7cf6"
      },
      "source": [
        "data = pd.read_csv(\"dataset.csv\")\n",
        "c_names = list(data[\"text\"][data[\"labels\"] == \"CompanyName\"])\n",
        "c_add = list(data[\"text\"][data[\"labels\"] == \"CompanyAdd\"])\n",
        "c_loc = list(data[\"text\"][data[\"labels\"] == \"CompanyLoc\"])\n",
        "c_serial = list(data[\"text\"][data[\"labels\"] == \"Serial\"])\n",
        "c_goods = list(data[\"text\"][data[\"labels\"] == \"Goods\"])\n",
        "print (data[\"labels\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CompanyName    4594492\n",
            "CompanyAdd     2601715\n",
            "CompanyLoc      114019\n",
            "Serial           17000\n",
            "Goods             9045\n",
            "Name: labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9td_UUzQMk4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(arr):\n",
        "    cleaned = []\n",
        "    for word in tqdm(arr):\n",
        "        word = word.lower()\n",
        "        word = re.sub(\"[^A-Za-z0-9&-|/]+\", \" \", word)\n",
        "        w_t = \" \".join([i for i in word_tokenize(word) if i not in stopset])\n",
        "        w_t = unidecode(w_t)\n",
        "        cleaned.append(w_t)\n",
        "    return cleaned\n",
        "\n",
        "def clean_word(word):\n",
        "    word = word.lower()\n",
        "    word = re.sub(\"[^A-Za-z0-9&-|/]+\", \" \", word)\n",
        "    w_t = \" \".join([i for i in word_tokenize(word) if i not in stopset])\n",
        "    w_t = unidecode(w_t)\n",
        "    return w_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OritUn-uVa0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_c_names = []\n",
        "print (\"Old Length:\", len(c_names))\n",
        "for c in tqdm(c_names):\n",
        "    if c[0] not in stopset and not c.startswith(\"and\") and c[0] not in [\"-\", \"&\", \"/\", \"|\"] and not c[0].isdigit() and len(c)>= 15:\n",
        "        new_c_names.append(c)\n",
        "c_names = new_c_names\n",
        "print (\"New Length:\", len(c_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wc366_DQJna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_names[:5], c_add[:5], c_loc[:5], c_serial[:5], c_goods[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFudHfVNiVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "c_names = clean(c_names)\n",
        "c_add = clean(c_add)\n",
        "c_loc = clean(c_loc)\n",
        "c_serial = clean(c_serial)\n",
        "c_goods = clean(c_goods)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfY6kmgiPffc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c_names[:5], c_add[:5], c_loc[:5], c_serial[:5], c_goods[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl2es71ObpvE",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alWj-zqzbqel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install bert-for-tf2\n",
        "! pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkDPk7Robq9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "import tensorflow_hub as hub\n",
        "from bert import bert_tokenization\n",
        "\n",
        "BertTokenizer = bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftWsdM4xbrDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "0539d013-ba9c-4db8-eb0f-e8af150381d8"
      },
      "source": [
        "def sample_token(arr):\n",
        "    for i in range(5):\n",
        "        token = tokenizer.tokenize(arr[i])\n",
        "        print (arr[i], token, tokenizer.convert_tokens_to_ids(token))\n",
        "\n",
        "sample_token(c_names)\n",
        "sample_token(c_add)\n",
        "sample_token(c_loc)\n",
        "sample_token(c_serial)\n",
        "sample_token(c_goods)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a m g engineering solutions limited ['a', 'm', 'g', 'engineering', 'solutions', 'limited'] [1037, 1049, 1043, 3330, 7300, 3132]\n",
            "a & a properties south coast limited ['a', '&', 'a', 'properties', 'south', 'coast', 'limited'] [1037, 1004, 1037, 5144, 2148, 3023, 3132]\n",
            "a & a property management services limited ['a', '&', 'a', 'property', 'management', 'services', 'limited'] [1037, 1004, 1037, 3200, 2968, 2578, 3132]\n",
            "a & m school of motoring limited ['a', '&', 'm', 'school', 'of', 'motor', '##ing', 'limited'] [1037, 1004, 1049, 2082, 1997, 5013, 2075, 3132]\n",
            "a & m swift ltd ['a', '&', 'm', 'swift', 'ltd'] [1037, 1004, 1049, 9170, 5183]\n",
            "metrohouse 57 pepper road hunslet leeds yorkshire ls10 2ru ['metro', '##house', '57', 'pepper', 'road', 'hu', '##ns', '##let', 'leeds', 'yorkshire', 'l', '##s', '##10', '2', '##ru'] [6005, 4580, 5401, 11565, 2346, 15876, 3619, 7485, 7873, 7018, 1048, 2015, 10790, 1016, 6820]\n",
            "the studio hatherlow house hatherlow romiley united kingdom sk6 3dy ['the', 'studio', 'hat', '##her', '##low', 'house', 'hat', '##her', '##low', 'rom', '##ile', '##y', 'united', 'kingdom', 'sk', '##6', '3d', '##y'] [1996, 2996, 6045, 5886, 8261, 2160, 6045, 5886, 8261, 17083, 9463, 2100, 2142, 2983, 15315, 2575, 7605, 2100]\n",
            "372 old street 335 rosden house london united kingdom ec1v 9lt ['37', '##2', 'old', 'street', '335', 'ro', '##sden', 'house', 'london', 'united', 'kingdom', 'ec', '##1', '##v', '9', '##lt'] [4261, 2475, 2214, 2395, 24426, 20996, 27903, 2160, 2414, 2142, 2983, 14925, 2487, 2615, 1023, 7096]\n",
            "95 marmion avenue chingford england e4 8ej ['95', 'mar', '##mi', '##on', 'avenue', 'ching', '##ford', 'england', 'e', '##4', '8', '##ej'] [5345, 9388, 4328, 2239, 3927, 19992, 3877, 2563, 1041, 2549, 1022, 20518]\n",
            "29 corry drive london united kingdom sw9 8qs ['29', 'co', '##rry', 'drive', 'london', 'united', 'kingdom', 'sw', '##9', '8', '##q', '##s'] [2756, 2522, 12244, 3298, 2414, 2142, 2983, 25430, 2683, 1022, 4160, 2015]\n",
            "leeds yorkshire ['leeds', 'yorkshire'] [7873, 7018]\n",
            "romiley united kingdom ['rom', '##ile', '##y', 'united', 'kingdom'] [17083, 9463, 2100, 2142, 2983]\n",
            "london united kingdom ['london', 'united', 'kingdom'] [2414, 2142, 2983]\n",
            "chingford england ['ching', '##ford', 'england'] [19992, 3877, 2563]\n",
            "edenbridge england ['eden', '##bridge', 'england'] [10267, 6374, 2563]\n",
            "71lhf|l6h|vy5b ['71', '##l', '##h', '##f', '|', 'l', '##6', '##h', '|', 'v', '##y', '##5', '##b'] [6390, 2140, 2232, 2546, 1064, 1048, 2575, 2232, 1064, 1058, 2100, 2629, 2497]\n",
            "215d|zc|fiyq ['215', '##d', '|', 'z', '##c', '|', 'fi', '##y', '##q'] [17405, 2094, 1064, 1062, 2278, 1064, 10882, 2100, 4160]\n",
            "sbh|irl|9dz ['sb', '##h', '|', 'ir', '##l', '|', '9', '##d', '##z'] [24829, 2232, 1064, 20868, 2140, 1064, 1023, 2094, 2480]\n",
            "65o6-cgp2-ty7x ['65', '##o', '##6', '-', 'c', '##gp', '##2', '-', 'ty', '##7', '##x'] [3515, 2080, 2575, 1011, 1039, 21600, 2475, 1011, 5939, 2581, 2595]\n",
            "5zir/acj4/ge ['5', '##zi', '##r', '/', 'ac', '##j', '##4', '/', 'ge'] [1019, 5831, 2099, 1013, 9353, 3501, 2549, 1013, 16216]\n",
            "clothing ['clothing'] [5929]\n",
            "women 's clothing ['women', \"'\", 's', 'clothing'] [2308, 1005, 1055, 5929]\n",
            "lingerie sleep & swimwear ['linger', '##ie', 'sleep', '&', 'swim', '##wear'] [26577, 2666, 3637, 1004, 9880, 16689]\n",
            "shorts ['shorts'] [9132]\n",
            "alisha shorts ['ali', '##sha', 'shorts'] [4862, 7377, 9132]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My3W90QzbrBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "pre_c_names = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in c_names]\n",
        "pre_c_add = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in c_add]\n",
        "pre_c_loc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in c_loc]\n",
        "pre_c_serial = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in c_serial]\n",
        "pre_c_goods = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in c_goods]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srYJdcZaAoK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4533482b-e706-4018-d23d-e326b75b9d17"
      },
      "source": [
        "%%time\n",
        "# sample 10,000 of each of classes\n",
        "import random\n",
        "random.seed(42)\n",
        "from functools import reduce\n",
        "import operator\n",
        "\n",
        "sample_c_names = random.sample(pre_c_names, 10000)\n",
        "sample_c_add = random.sample(pre_c_add, 10000)\n",
        "sample_c_loc = random.sample(pre_c_loc, 10000)\n",
        "sample_c_serial = random.sample(pre_c_serial, 10000)\n",
        "sample_c_goods = random.sample(pre_c_goods, len(pre_c_goods))\n",
        "\n",
        "x = []\n",
        "for i in [sample_c_names, sample_c_add, sample_c_loc, sample_c_serial, sample_c_goods]:\n",
        "    for j in i:\n",
        "        x.append(j)\n",
        "\n",
        "labels = [\"CompanyName\"]*len(sample_c_names)\n",
        "labels.extend([\"CompanyAdd\"]*len(sample_c_add))\n",
        "labels.extend([\"CompanyLoc\"]*len(sample_c_loc))\n",
        "labels.extend([\"Serial\"]*len(sample_c_serial))\n",
        "labels.extend([\"Goods\"]*len(sample_c_goods))\n",
        "print (len(x), len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49045 49045\n",
            "CPU times: user 68.3 ms, sys: 994 Âµs, total: 69.3 ms\n",
            "Wall time: 68.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnhhG7LLudwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df997084-1ea0-40c7-cdf5-232b2bc4c537"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "ohe = OneHotEncoder()\n",
        "y = ohe.fit_transform(np.array(labels).reshape(-1, 1))\n",
        "y = y.toarray()\n",
        "classes = list(ohe.categories_[0])\n",
        "print (classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CompanyAdd', 'CompanyLoc', 'CompanyName', 'Goods', 'Serial']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8kIp4HQ5n5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Following this : https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "dataset = [[x[i], y[i], len(x)] for i in range(len(x))]\n",
        "random.shuffle(dataset)\n",
        "dataset.sort(key=lambda x: x[2])\n",
        "sorted_dataset = [(d[0], d[1]) for d in dataset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGUaKAo5qhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_dataset, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x5fEApR5qWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9b3da6c1-7875-4b4c-b2d4-da805694546e"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
        "next(iter(batched_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(256, 28), dtype=int32, numpy=\n",
              " array([[ 1041,  9096, 18098, ...,     0,     0,     0],\n",
              "        [ 9874,  2100,  3057, ...,     0,     0,     0],\n",
              "        [ 1054,  3501,  2102, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [ 9779,  5974, 25545, ...,     0,     0,     0],\n",
              "        [22851,  8973,  8862, ...,     0,     0,     0],\n",
              "        [ 5170,  3270,  8953, ...,     0,     0,     0]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(256, 5), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1],\n",
              "        ...,\n",
              "        [0, 0, 0, 1, 0],\n",
              "        [0, 0, 1, 0, 0],\n",
              "        [1, 0, 0, 0, 0]], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPBk8Mpy7hkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13f326d0-e46a-4f34-aa36-73743b470bad"
      },
      "source": [
        "TOTAL_BATCHES = int(len(sorted_dataset) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)\n",
        "print (TOTAL_BATCHES*BATCH_SIZE, TEST_BATCHES*BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48896 4864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bs6JHb9tbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=5,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size, embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.last_dense = layers.Dense(units=model_output_classes, activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLqA5ffL-K5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 5\n",
        "DROPOUT_RATE = 0.4\n",
        "NB_EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXNmyu5h-ODQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "text_model.compile(loss=\"categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4vJJj4I-9id",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c9a24764-9385-4131-bbe5-14d67fc831c7"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "173/173 [==============================] - 44s 253ms/step - loss: 0.2557 - categorical_accuracy: 0.9270\n",
            "Epoch 2/2\n",
            "173/173 [==============================] - 44s 253ms/step - loss: 0.0142 - categorical_accuracy: 0.9963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa49bdbb860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiM5LBZG_BFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a284273e-ddbb-420e-be1d-5f8fe4ed3a79"
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 1s 77ms/step - loss: 0.0289 - categorical_accuracy: 0.9914\n",
            "[0.028882566839456558, 0.9913651347160339]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYWjwy3F_GEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "5c87c91f-5c96-4395-8e20-8a1265eb36d6"
      },
      "source": [
        "strings = [\"MARKS AND SPENCERS LTD\", \"INTEL Corporation LLC\", \"M&S LTD\", \"Microsoft Corporation\", \"XYZ 13423/ILD\",\n",
        "           \"ABC/ICL/20891NC\", \"ICNAO02312\", \"LONDON\", \"LONDON, GREAT BRITAIN\", \"LONDON, ENGLAND\",\n",
        "           \"SLOUGH SE12 2XY\", \"33 TIMBER YARD, LONDON, L1 8XY\", \"44 CHINA ROAD, KOWLOON, HONG KONG\",\n",
        "           \"HARDWOOD TABLE\", \"PLASTIC BOTTLE\", \"TOYS\", ]\n",
        "gt = [\"CompanyName\", \"CompanyName\", \"CompanyName\", \"CompanyName\", \n",
        "      \"Serial\", \"Serial\", \"Serial\",  \n",
        "      \"CompanyLoc\", \"CompanyLoc\", \"CompanyLoc\", \n",
        "      \"CompanyAdd\", \"CompanyAdd\", \"CompanyAdd\",\n",
        "      \"Goods\", \"Goods\", \"Goods\"]\n",
        "\n",
        "for i, x in enumerate(strings):\n",
        "    cleaned = clean_word(x)\n",
        "    tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))\n",
        "    if len(tokens) < 20:\n",
        "        tokens.extend([0]*(20-len(tokens)))\n",
        "    preds = text_model(tf.expand_dims(tokens, 0), training=False)\n",
        "    print (\"String:\", x, \"Output:\", preds[0][np.argmax(preds)]*100)\n",
        "    print (\"Prediction:\", classes[np.argmax(preds)], \"Truth:\", gt[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "String: MARKS AND SPENCERS LTD Output: tf.Tensor(99.99875, shape=(), dtype=float32)\n",
            "Prediction: CompanyName Truth: CompanyName\n",
            "String: INTEL Corporation LLC Output: tf.Tensor(80.18121, shape=(), dtype=float32)\n",
            "Prediction: CompanyName Truth: CompanyName\n",
            "String: M&S LTD Output: tf.Tensor(99.99949, shape=(), dtype=float32)\n",
            "Prediction: CompanyName Truth: CompanyName\n",
            "String: Microsoft Corporation Output: tf.Tensor(86.59512, shape=(), dtype=float32)\n",
            "Prediction: CompanyName Truth: CompanyName\n",
            "String: XYZ 13423/ILD Output: tf.Tensor(99.96125, shape=(), dtype=float32)\n",
            "Prediction: Serial Truth: Serial\n",
            "String: ABC/ICL/20891NC Output: tf.Tensor(99.985374, shape=(), dtype=float32)\n",
            "Prediction: Serial Truth: Serial\n",
            "String: ICNAO02312 Output: tf.Tensor(99.51053, shape=(), dtype=float32)\n",
            "Prediction: Serial Truth: Serial\n",
            "String: LONDON Output: tf.Tensor(99.748405, shape=(), dtype=float32)\n",
            "Prediction: CompanyLoc Truth: CompanyLoc\n",
            "String: LONDON, GREAT BRITAIN Output: tf.Tensor(99.838104, shape=(), dtype=float32)\n",
            "Prediction: CompanyLoc Truth: CompanyLoc\n",
            "String: LONDON, ENGLAND Output: tf.Tensor(99.998405, shape=(), dtype=float32)\n",
            "Prediction: CompanyLoc Truth: CompanyLoc\n",
            "String: SLOUGH SE12 2XY Output: tf.Tensor(99.94628, shape=(), dtype=float32)\n",
            "Prediction: CompanyAdd Truth: CompanyAdd\n",
            "String: 33 TIMBER YARD, LONDON, L1 8XY Output: tf.Tensor(99.932884, shape=(), dtype=float32)\n",
            "Prediction: CompanyAdd Truth: CompanyAdd\n",
            "String: 44 CHINA ROAD, KOWLOON, HONG KONG Output: tf.Tensor(83.71971, shape=(), dtype=float32)\n",
            "Prediction: CompanyLoc Truth: CompanyAdd\n",
            "String: HARDWOOD TABLE Output: tf.Tensor(99.741745, shape=(), dtype=float32)\n",
            "Prediction: Goods Truth: Goods\n",
            "String: PLASTIC BOTTLE Output: tf.Tensor(99.957436, shape=(), dtype=float32)\n",
            "Prediction: Goods Truth: Goods\n",
            "String: TOYS Output: tf.Tensor(99.61712, shape=(), dtype=float32)\n",
            "Prediction: Goods Truth: Goods\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2G_GMEftS1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}